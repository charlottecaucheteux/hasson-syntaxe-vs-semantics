{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "grateful-committee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-berlin",
   "metadata": {},
   "source": [
    "# Pairwise correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "rough-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import permutation\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "X = np.random.rand(1000, 50000)\n",
    "Y = X**2\n",
    "zero = np.random.rand(1000, 50000)\n",
    "test = pairwise_distances(X, Y, ) #.mean() #\"correlation\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "acknowledged-schedule",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid\n",
      "CPU times: user 2.46 s, sys: 0 ns, total: 2.46 s\n",
      "Wall time: 2.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "v2v(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "listed-wagner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "documented-advertiser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid\n",
      "CPU times: user 1.62 s, sys: 964 Âµs, total: 1.62 s\n",
      "Wall time: 1.61 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "v2v_1d(X, Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "mysterious-brisbane",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid\n",
      "CPU times: user 2.57 s, sys: 0 ns, total: 2.57 s\n",
      "Wall time: 2.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "v2v(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "removable-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import permutation\n",
    "\n",
    "def v2v_old(true, pred, metric=\"cosine\"):\n",
    "    r = pairwise_distances(true, pred, metric)\n",
    "    ns = len(r)\n",
    "    pick = np.random.choice(ns, ns)\n",
    "    invalid = (pick==np.arange(ns))\n",
    "    while invalid.any():\n",
    "        print(\"invalid\")\n",
    "        pick[invalid] = np.random.choice(invalid.sum())\n",
    "        invalid = (pick==np.arange)\n",
    "    correct = np.diag(r) < r[np.arange(ns), pick]\n",
    "    acc = correct.sum() / len(correct) \n",
    "    return acc\n",
    "\n",
    "\n",
    "def v2v_1d(true, pred, metric=\"cosine\"):\n",
    "    r = np.abs(true[None] - pred[:, None])\n",
    "    ns = len(r)\n",
    "    pick = np.random.choice(ns, ns)\n",
    "    invalid = (pick==np.arange(ns))\n",
    "    while invalid.any():\n",
    "        print(\"invalid\")\n",
    "        pick[invalid] = np.random.choice(invalid.sum())\n",
    "        invalid = (pick==np.arange)\n",
    "    diag = np.arange(ns)\n",
    "    correct = r[diag, diag] < r[diag, pick] # of shape (1000, 100)\n",
    "    acc = correct.sum(0) / correct.shape[-1]\n",
    "    return acc\n",
    "\n",
    "\n",
    "def v2v(true, pred, metric=\"cosine\"):\n",
    "    assert len(true) == len(pred)\n",
    "    ns = len(true)\n",
    "    first = permutation(ns)\n",
    "    second = permutation(ns)\n",
    "    while (first==second).any():\n",
    "        print(\"invalid\")\n",
    "        first[first==second] = np.random.choice((first==second).sum())\n",
    "    \n",
    "    correct = 0\n",
    "    for i, j in zip(first, second):\n",
    "        r = pairwise_distances(true[[i, j]], pred[[i, j]], metric)\n",
    "        diag = np.diag(r).sum()\n",
    "        correct += 1*(diag < r.sum() - diag)\n",
    "    acc = correct / ns\n",
    "    return acc\n",
    "\n",
    "\n",
    "def v2v_1d(true, pred):\n",
    "    assert len(true) == len(pred)\n",
    "    ns = len(true)\n",
    "    first = permutation(ns)\n",
    "    second = permutation(ns)\n",
    "    while (first==second).any():\n",
    "        print(\"invalid\")\n",
    "        first[first==second] = np.random.choice((first==second).sum())\n",
    "    \n",
    "    correct = 0\n",
    "    for i, j in zip(first, second):\n",
    "        r = np.abs(true[[i, j]][None] - pred[[i, j]][:, None])\n",
    "        diag = r[0, 0] + r[1, 1]\n",
    "        correct += 1*(diag < r.sum((0, 1)) - diag)\n",
    "    acc = correct / ns\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "veterinary-acrobat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 8, 6, 5, 7, 9, 1, 2, 0])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2v(true, pred, metric=correlate):\n",
    "    ns = len(true)\n",
    "    pick = np.random.choice(ns, ns)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-reply",
   "metadata": {},
   "source": [
    "# New models (\"bert\" \"T5\")\n",
    "\n",
    "\n",
    "\n",
    "* bert-base-uncased\n",
    "* squeezebert/squeezebert-mnli\n",
    "* xlnet-base-cased\n",
    "* roberta-base\n",
    "* transfo-xl-wt103\n",
    "* allenai/longformer-base-4096\n",
    "* t5-base\n",
    "* DialoGPT-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "expected-attack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-uncased',\n",
       " 'squeezebert/squeezebert-mnli',\n",
       " 'xlnet-base-cased',\n",
       " 'roberta-base',\n",
       " 'transfo-xl-wt103',\n",
       " 'allenai/longformer-base-4096',\n",
       " 't5-base',\n",
       " 'DialoGPT-small']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"* bert-base-uncased\n",
    "* squeezebert/squeezebert-mnli\n",
    "* xlnet-base-cased\n",
    "* roberta-base\n",
    "* transfo-xl-wt103\n",
    "* allenai/longformer-base-4096\n",
    "* t5-base\n",
    "* DialoGPT-small\"\"\".replace(\"* \", \"\").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-villa",
   "metadata": {},
   "source": [
    "*distilbert\n",
    "*bert-base-multilingual-uncased (to compare the effect of learning on other language)\n",
    "*gpt2-medium\n",
    "*gpt2-large (?)\n",
    "*xlnet-base-cased\n",
    "*roberta-base\n",
    "*transfo-xl-wt103\n",
    "*t5-base\n",
    "\n",
    "*albert-base-v1\n",
    "*DialoGPT-small\n",
    "*roberta-base-openai-detector\n",
    "*distilbert-base-uncased\n",
    "*distilgpt2\n",
    "*microsoft/layoutlm-base-uncased\n",
    "*allenai/longformer-base-4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "stupid-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def extract_causal_hidden(model, input_ids):\n",
    "    assert input_ids.size(0) == 1\n",
    "    nw = input_ids.size(1)\n",
    "    input_ids = input_ids.expand((nw, nw))\n",
    "    mask = 1-torch.triu(torch.ones_like(input_ids))\n",
    "    outputs = model(input_ids, attention_mask=mask, output_hidden_states=True)\n",
    "    outputs = outputs.hidden_states\n",
    "    outputs = [out[torch.arange(nw), torch.arange(nw)][None] for out in outputs]\n",
    "    print(outputs[0].shape, \"extract_causal_hidden\")\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def yield_hidden_states(model, inputs, max_len=256):\n",
    "    idx = np.arange(inputs[\"input_ids\"].size(1))\n",
    "    splits = np.array_split(idx, len(idx) // max_len + 1)\n",
    "    for idx in splits:\n",
    "        batch_inputs = {k: v[:, idx] for k, v in inputs.items()}\n",
    "        outputs = model(**batch_inputs, output_hidden_states=True)\n",
    "\n",
    "        # FIX\n",
    "        outputs = list(outputs[2])\n",
    "        outputs[0] = model.base_model.wte.forward(batch_inputs[\"input_ids\"])\n",
    "        print(\"HERE OK \")\n",
    "        hidden_states = torch.stack(outputs).squeeze(1)\n",
    "        yield hidden_states\n",
    "\n",
    "def yield_hidden_states_general(model, inputs, max_len=256, \n",
    "                                force_causal=False, fix_embeddings=True):\n",
    "    inputs = inputs[\"input_ids\"]\n",
    "    assert inputs.size(0) == 1 \n",
    "    idx = np.arange(inputs.size(1)) #np.arange(inputs[\"input_ids\"].size(1))\n",
    "    splits = np.array_split(idx, len(idx) // max_len + 1)\n",
    "    with torch.no_grad():\n",
    "        for idx in splits:\n",
    "\n",
    "            batch_inputs = inputs[:, idx].clone()\n",
    "\n",
    "            if force_causal:\n",
    "                nw = len(idx)\n",
    "                batch_inputs = batch_inputs.expand((nw, nw))\n",
    "                mask = 1-torch.triu(torch.ones_like(batch_inputs))\n",
    "                out = model(batch_inputs, output_hidden_states=True, attention_mask=mask)\n",
    "                out = out.hidden_states\n",
    "                out = [x[torch.arange(nw), torch.arange(nw)][None] for x in out]\n",
    "\n",
    "            else:\n",
    "                out = model(batch_inputs, output_hidden_states=True)\n",
    "                out = list(out.hidden_states)   \n",
    "\n",
    "            if fix_embeddings: # TOFIX, not clean, not general\n",
    "                out[0] = model.base_model.wte.forward(batch_inputs)\n",
    "\n",
    "            hidden_states = torch.stack(out).squeeze(1)\n",
    "            yield hidden_states\n",
    "\n",
    "\n",
    "def map_word_to_inputs(words, tokenizer):\n",
    "    mapping = {}\n",
    "    idx = 0\n",
    "    inputs = tokenizer(\"\", return_tensors=\"pt\", add_special_tokens=False)\n",
    "    inputs = {k: v.long() for k, v in inputs.items()}\n",
    "    for i, word in enumerate(words):\n",
    "        word_inpt = tokenizer(word, return_tensors=\"pt\")\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.cat([inputs[k], word_inpt[k]], dim=1).long()\n",
    "        ntok = word_inpt[k].size(1)\n",
    "        mapping[i] = torch.arange(idx, idx + ntok + 1)\n",
    "        idx += ntok\n",
    "    return inputs, mapping\n",
    "\n",
    "\n",
    "def get_transformer_embeddings(words, model_name=\"gpt2\", agg=\"mean\"):\n",
    "\n",
    "    # Load\n",
    "    # words = events.word_raw.values\n",
    "    assert agg in [\"sum\", \"mean\", \"last\"]\n",
    "\n",
    "    print(f\"Loading model {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    if model_name == \"gpt2\":\n",
    "        opts = dict(force_causal=False, fix_embeddings=True)\n",
    "    else:\n",
    "        opts = dict(force_causal=True, fix_embeddings=False)\n",
    "        \n",
    "\n",
    "    # Inputs\n",
    "    print(\"Processed words\", words[:5])\n",
    "    inputs, mapping = map_word_to_inputs(words, tokenizer)\n",
    "    # import pdb\n",
    "\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        hidden_states = torch.cat(list(yield_hidden_states_general(model, inputs, **opts)), dim=1)\n",
    "\n",
    "        if agg == \"mean\":\n",
    "            logging.warning(\"Averaging BPE\")\n",
    "            # Mapping\n",
    "            word_level_hidden_states = torch.stack(\n",
    "                [\n",
    "                    torch.mean(hidden_states[:, mapping[i][0] : mapping[i][-1]], dim=1)\n",
    "                    for i in range(len(words))\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        elif agg == \"sum\":\n",
    "            logging.warning(\"Summing BPE\")\n",
    "            # Mapping\n",
    "            word_level_hidden_states = torch.stack(\n",
    "                [\n",
    "                    torch.sum(hidden_states[:, mapping[i][0] : mapping[i][-1]], dim=1)\n",
    "                    for i in range(len(words))\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        elif agg == \"last\":\n",
    "            logging.warning(\"Cutting to the last BPE\")\n",
    "            # Mapping\n",
    "            word_level_hidden_states = torch.stack(\n",
    "                [hidden_states[:, (mapping[i][-1] - 1)] for i in range(len(words))],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "    assert word_level_hidden_states.size(1) == len(words)\n",
    "    return word_level_hidden_states.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "front-cement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"I\", \"am\", \"a\", \"cat\"]\n",
    "map_word_to_inputs(words, tokenizer)[0][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "destroyed-showcase",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "union-adventure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE OK \n"
     ]
    }
   ],
   "source": [
    "out = torch.cat(list(yield_hidden_states(model, input_ids)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "prerequisite-spectacular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.1'"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "sitting-headline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 4, 768])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-westminster",
   "metadata": {},
   "source": [
    "# Select models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "roman-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"albert-base-v1\",\n",
    "#\"DialoGPT-small\",\n",
    "\"roberta-base-openai-detector\",\n",
    "\"distilbert-base-uncased\",\n",
    "\"distilgpt2\",\n",
    "\"microsoft/layoutlm-base-uncased\",\n",
    "\"allenai/longformer-base-4096\"]\n",
    "\n",
    "\n",
    "models += ['bert-base-uncased',\n",
    " 'squeezebert/squeezebert-mnli',\n",
    " 'xlnet-base-cased',\n",
    " 'roberta-base',\n",
    " 'transfo-xl-wt103',\n",
    " 'allenai/longformer-base-4096',\n",
    " #'t5-base',\n",
    " #'DialoGPT-small',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-christianity",
   "metadata": {},
   "source": [
    "# Check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "thick-typing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/ccaucheteux/hasson-syntaxe-vs-semantics\n"
     ]
    }
   ],
   "source": [
    "cd /private/home/ccaucheteux/hasson-syntaxe-vs-semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "adjustable-tuner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /public/apps/anaconda3/2020.11/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "__init__.py                     get_control_features.py\r\n",
      "\u001b[0m\u001b[01;34m__pycache__\u001b[0m/                    get_features.py\r\n",
      "constants.py                    get_features_old.py\r\n",
      "encode.py                       get_lexical_phone_error.py\r\n",
      "exp_multisubjects.py            manning_proj_embeddings.py\r\n",
      "exp_multitasks.py               paths.py\r\n",
      "exp_single_task.py              preprocess_stim.py\r\n",
      "exp_singlesubject_mutifeats.py  syntactic_equivalences.py\r\n",
      "fir.py                          task_dataset.py\r\n",
      "fmri_old.py                     transformer_embeddings.py\r\n",
      "get_bold.py                     transformer_errors.py\r\n",
      "get_concat_features.py          \u001b[01;34mutils\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "floral-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess_stim import get_stimulus\n",
    "stim = get_stimulus(\"pieman\", lower=False)\n",
    "tokens = stim.word_raw.values[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "terminal-manor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert-base-v1\n",
      "Loading model albert-base-v1\n",
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summing BPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 600, 768])\n",
      "roberta-base-openai-detector\n",
      "Loading model roberta-base-openai-detector\n",
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summing BPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 600, 768])\n",
      "distilbert-base-uncased\n",
      "Loading model distilbert-base-uncased\n",
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summing BPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 600, 768])\n",
      "distilgpt2\n",
      "Loading model distilgpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summing BPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 600, 768])\n",
      "microsoft/layoutlm-base-uncased\n",
      "Loading model microsoft/layoutlm-base-uncased\n",
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summing BPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 600, 768])\n",
      "allenai/longformer-base-4096\n",
      "Loading model allenai/longformer-base-4096\n",
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summing BPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 600, 768])\n",
      "bert-base-uncased\n",
      "Loading model bert-base-uncased\n",
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summing BPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 600, 768])\n",
      "squeezebert/squeezebert-mnli\n",
      "Loading model squeezebert/squeezebert-mnli\n",
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summing BPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 600, 768])\n",
      "xlnet-base-cased\n",
      "Loading model xlnet-base-cased\n",
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summing BPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 600, 768])\n",
      "roberta-base\n",
      "Loading model roberta-base\n",
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Summing BPE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 600, 768])\n",
      "transfo-xl-wt103\n",
      "Loading model transfo-xl-wt103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/ccaucheteux/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed words ['I' 'began' 'my' 'illustrious' 'career']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-31de7d4433e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_transformer_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-205d47977a3a>\u001b[0m in \u001b[0;36mget_transformer_embeddings\u001b[0;34m(words, model_name, agg)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myield_hidden_states_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0magg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-205d47977a3a>\u001b[0m in \u001b[0;36myield_hidden_states_general\u001b[0;34m(model, inputs, max_len, force_causal, fix_embeddings)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mbatch_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'attention_mask'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "causals = []\n",
    "errors = []\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    with torch.no_grad():\n",
    "        emb = get_transformer_embeddings(tokens, model_name, agg=\"sum\")\n",
    "        print(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-breathing",
   "metadata": {},
   "source": [
    "# Check causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "least-ceramic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert-base-v1\n",
      "Unrecognized configuration class <class 'transformers.models.albert.configuration_albert.AlbertConfig'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "Model type should be one of CamembertConfig, XLMRobertaConfig, RobertaConfig, BertConfig, OpenAIGPTConfig, GPT2Config, TransfoXLConfig, XLNetConfig, XLMConfig, CTRLConfig, ReformerConfig, BertGenerationConfig, XLMProphetNetConfig, ProphetNetConfig. causalLM\n",
      "n_splits 1\n",
      "[0 1 2 3]\n",
      "tensor([[10, 11, 12, 20]])\n",
      "out.shape torch.Size([13, 4, 768])\n",
      "out.shape torch.Size([768])\n",
      "n_splits 1\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "tensor([[10, 11, 12, 20, 10, 11,  5,  2,  5]])\n",
      "out2.shape torch.Size([13, 9, 768])\n",
      "out2.shape torch.Size([768])\n",
      "False\n",
      "roberta-base-openai-detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-f4bfa7285a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Okay for AutoModelForCausalLM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODEL_FOR_CAUSAL_LM_MAPPING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             return MODEL_FOR_CAUSAL_LM_MAPPING[type(config)].from_pretrained(\n\u001b[0m\u001b[1;32m   1029\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_pooling_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaLMHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaPooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0madd_pooling_layer\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36minit_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \"\"\"\n\u001b[1;32m    749\u001b[0m         \u001b[0;31m# Initialize weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;31m# Prune heads if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \"\"\"\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \"\"\"\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36m_init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;31m# Slightly different from the TF version which uses truncated_normal for initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;31m# cf https://github.com/pytorch/pytorch/pull/5617\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "causals = []\n",
    "errors = []\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        print(\"Okay for AutoModelForCausalLM\")\n",
    "    except Exception as e:\n",
    "        print(e, \"causalLM\")\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                model = AutoModel.from_pretrained(model_name)\n",
    "            except Exception as e:\n",
    "                print(e, \"automodel\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.LongTensor([[10, 11, 12, 20]])\n",
    "        input_ids = {\"input_ids\":input_ids}\n",
    "        \n",
    "        #out = extract_causal_hidden(model, input_ids)\n",
    "        #out = out[1][0, 2]\n",
    "        \n",
    "        opts = dict(force_causal=True, fix_embeddings=False)\n",
    "        out = torch.cat(list(yield_hidden_states_general(model, input_ids, **opts)), dim=1)\n",
    "        print(\"out.shape\", out.shape)\n",
    "        out = out[1, 2]\n",
    "        print(\"out.shape\", out.shape)\n",
    "        \n",
    "        input_ids = torch.LongTensor([[10, 11, 12, 20, 10, 11, 5, 2, 5]])\n",
    "        input_ids = {\"input_ids\":input_ids}\n",
    "        \n",
    "        #out2 = extract_causal_hidden(model, input_ids)\n",
    "        #out2 = out2[1][0, 2]\n",
    "        \n",
    "        out2 = torch.cat(list(yield_hidden_states_general(model, input_ids, **opts)), dim=1)\n",
    "        print(\"out2.shape\", out2.shape)\n",
    "        out2 = out2[1, 2]\n",
    "        print(\"out2.shape\", out2.shape)\n",
    "\n",
    "        print(torch.allclose(out, out2))\n",
    "        if torch.allclose(out, out2, atol=1e-3, rtol=1e-3):\n",
    "            causals.append(model_name)\n",
    "        else:\n",
    "            errors.append(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "special-kansas",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9056,  0.2749,  2.0964,  ...,  0.6398, -0.0260, -0.1634],\n",
       "         [-0.5949, -0.0208,  0.4261,  ..., -0.1078,  1.3618,  0.5799],\n",
       "         [-0.3211,  0.0869, -1.1684,  ...,  0.3524,  0.6197,  0.2317],\n",
       "         ...,\n",
       "         [-0.3214,  0.1721,  0.4360,  ...,  0.4868, -0.6630,  1.0812],\n",
       "         [-0.1438,  0.2584,  1.0188,  ...,  1.3151,  0.5453,  0.1916],\n",
       "         [-0.0418,  0.5118,  0.9573,  ...,  0.4976, -0.4530,  1.0069]],\n",
       "\n",
       "        [[-0.8079,  0.4702,  2.0856,  ...,  0.6243,  0.9628, -0.7691],\n",
       "         [ 0.1957, -0.5334,  0.4716,  ..., -0.0964,  0.9694, -0.1964],\n",
       "         [-0.1026,  0.2669, -1.3320,  ...,  0.8108,  0.2486,  0.1447],\n",
       "         ...,\n",
       "         [ 0.6706,  0.0726,  0.6357,  ...,  0.5045,  0.1864,  1.6206],\n",
       "         [ 0.6126,  0.1873,  0.8639,  ...,  0.9788,  0.3206,  0.5816],\n",
       "         [ 0.3499,  0.2055,  0.9875,  ...,  0.6285,  0.6360,  1.1422]],\n",
       "\n",
       "        [[-0.4876,  0.2747,  1.6616,  ...,  0.3690,  1.1764, -0.5214],\n",
       "         [ 0.8741, -1.0890,  0.6164,  ...,  0.1645,  0.8589, -0.0825],\n",
       "         [-0.1103,  0.0172, -0.7564,  ...,  1.5172, -0.2108, -0.5429],\n",
       "         ...,\n",
       "         [ 0.7510,  0.1913,  0.5457,  ...,  0.7928,  0.4161,  1.2872],\n",
       "         [ 0.6394,  0.1458,  0.6381,  ...,  0.9683,  0.5043,  0.5339],\n",
       "         [ 0.8673,  0.3328,  0.8468,  ...,  0.5977,  0.5898,  1.6214]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.8215, -0.3650,  2.4391,  ..., -1.0628,  0.0420, -0.6808],\n",
       "         [ 0.5488, -0.7744,  1.3770,  ..., -1.3550, -0.4692, -0.0729],\n",
       "         [-0.3616, -0.7606,  1.1630,  ..., -0.8895, -0.4520, -0.2868],\n",
       "         ...,\n",
       "         [ 0.0358, -0.5645, -0.0335,  ..., -0.8792, -0.9966, -0.0538],\n",
       "         [ 0.7138,  0.0578,  1.6301,  ..., -1.0335, -0.9537, -0.9034],\n",
       "         [ 0.0519, -0.7670,  0.8320,  ..., -0.3190, -0.6414, -1.3326]],\n",
       "\n",
       "        [[-0.1348, -0.6073,  2.1481,  ..., -0.2979, -0.4446, -0.8764],\n",
       "         [-0.6036, -0.3649,  1.3771,  ..., -1.3799, -0.6978, -0.3858],\n",
       "         [-0.9993, -0.5395,  1.3807,  ..., -0.5878, -0.8736, -0.5616],\n",
       "         ...,\n",
       "         [-0.8603, -0.3022, -0.4524,  ..., -0.3835, -0.8359, -0.1400],\n",
       "         [ 0.1593, -0.5962,  1.2908,  ..., -0.8120, -1.1429, -0.7995],\n",
       "         [-0.2464, -0.8452,  0.4282,  ..., -0.4447, -0.5611, -1.1551]],\n",
       "\n",
       "        [[-0.8207, -0.4462,  1.7623,  ..., -0.2201, -0.9640, -0.9182],\n",
       "         [-1.3687, -0.1454,  1.4939,  ..., -0.6030, -0.1178, -0.4104],\n",
       "         [-1.2831, -0.2657,  1.0594,  ..., -0.2295, -0.7864, -1.0776],\n",
       "         ...,\n",
       "         [-1.0577,  0.5438, -0.8163,  ..., -0.2579, -0.6233, -0.8365],\n",
       "         [-0.6851, -0.8732,  0.9409,  ..., -0.6237, -1.2489, -0.5585],\n",
       "         [-0.5815, -0.3859, -0.2530,  ..., -0.2789, -0.5498, -1.3993]]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts = dict(force_causal=True, fix_embeddings=False)\n",
    "opts = dict(force_causal=True, fix_embeddings=False)\n",
    "out = torch.cat(list(yield_hidden_states_general(model, input_ids, **opts)), dim=1)\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hasson",
   "language": "python",
   "name": "hasson"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
