{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mighty-toronto",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/ccaucheteux/hasson-syntaxe-vs-semantics\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "public-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess_stim import get_phone_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "qualified-smooth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aa_B', 'aa_E', 'aa_I', 'aa_S', 'ae_B', 'ae_E', 'ae_I', 'ah_B', 'ah_E', 'ah_I', 'ah_S', 'ao_B', 'ao_E', 'ao_I', 'aw_B', 'aw_E', 'aw_I', 'ay_B', 'ay_E', 'ay_I', 'ay_S', 'b_B', 'b_E', 'b_I', 'ch_B', 'ch_E', 'ch_I', 'd_B', 'd_E', 'd_I', 'dh_B', 'dh_E', 'dh_I', 'eh_B', 'eh_I', 'eh_S', 'er_B', 'er_E', 'er_I', 'er_S', 'ey_B', 'ey_E', 'ey_I', 'ey_S', 'f_B', 'f_E', 'f_I', 'g_B', 'g_E', 'g_I', 'hh_B', 'hh_I', 'ih_B', 'ih_E', 'ih_I', 'iy_B', 'iy_E', 'iy_I', 'iy_S', 'jh_B', 'jh_E', 'jh_I', 'k_B', 'k_E', 'k_I', 'l_B', 'l_E', 'l_I', 'm_B', 'm_E', 'm_I', 'n_B', 'n_E', 'n_I', 'ng_E', 'ng_I', 'oov_S', 'ow_B', 'ow_E', 'ow_I', 'ow_S', 'oy_E', 'oy_I', 'p_B', 'p_E', 'p_I', 'r_B', 'r_E', 'r_I', 's_B', 's_E', 's_I', 'sh_B', 'sh_E', 'sh_I', 't_B', 't_E', 't_I', 'th_B', 'th_E', 'th_I', 'uh_I', 'uw_E', 'uw_I', 'v_B', 'v_E', 'v_I', 'w_B', 'w_I', 'y_B', 'y_I', 'z_B', 'z_E', 'z_I', 'zh_E', 'zh_I', ''])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_phone_dic().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-arkansas",
   "metadata": {},
   "source": [
    "# Checks lexicality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "derived-battery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.1'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "valuable-plate",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9e9dfe027206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fresh-knight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.35975"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9439/100*1.5/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "committed-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "def map_word_to_inputs(words, tokenizer):\n",
    "    mapping = {}\n",
    "    idx = 0\n",
    "    inputs = tokenizer(\"\", return_tensors=\"pt\", add_special_tokens=False)\n",
    "    inputs = {k: v.long() for k, v in inputs.items()}\n",
    "    for i, word in enumerate(words):\n",
    "        word_inpt = tokenizer(word, return_tensors=\"pt\")\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.cat([inputs[k], word_inpt[k]], dim=1).long()\n",
    "        ntok = word_inpt[k].size(1)\n",
    "        mapping[i] = torch.arange(idx, idx + ntok + 1)\n",
    "        idx += ntok\n",
    "    return inputs, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "typical-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/ccaucheteux/hasson-syntaxe-vs-semantics\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "premium-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "preliminary-sacramento",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21styear',\n",
       " 'bigbang',\n",
       " 'black',\n",
       " 'bronx',\n",
       " 'forgot',\n",
       " 'friends',\n",
       " 'himym',\n",
       " 'lucy',\n",
       " 'merlin',\n",
       " 'milkywayoriginal',\n",
       " 'milkywaysynonyms',\n",
       " 'milkywayvodka',\n",
       " 'notthefallintact',\n",
       " 'pieman',\n",
       " 'piemanpni',\n",
       " 'prettymouth',\n",
       " 'santa',\n",
       " 'seinfeld',\n",
       " 'shame',\n",
       " 'shapesphysical',\n",
       " 'shapessocial',\n",
       " 'sherlock',\n",
       " 'slumlordreach',\n",
       " 'tunnel',\n",
       " 'upintheair',\n",
       " 'vinny']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_files = list(paths.gentle_path.glob(\"*/transcript.txt\"))\n",
    "tasks = [p.parent.name for p in transcript_files]\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "authentic-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_tokenizer = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "patient-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess_stim import format_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "elder-dylan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Test is on the nat"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer(\"Test is on the nat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "comic-broadcasting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in mapping.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "forbidden-combining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21styear\n",
      "bigbang\n",
      "black\n",
      "bronx\n",
      "forgot\n",
      "friends\n",
      "himym\n",
      "lucy\n",
      "merlin\n",
      "milkywayoriginal\n",
      "milkywaysynonyms\n",
      "milkywayvodka\n",
      "notthefallintact\n",
      "pieman\n",
      "piemanpni\n",
      "prettymouth\n",
      "santa\n",
      "seinfeld\n",
      "shame\n",
      "shapesphysical\n",
      "shapessocial\n",
      "sherlock\n",
      "slumlordreach\n",
      "tunnel\n",
      "upintheair\n",
      "vinny\n"
     ]
    }
   ],
   "source": [
    "model_name=\"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "lexical = {}\n",
    "for file, task in zip(transcript_files, tasks):\n",
    "    print(task)\n",
    "    transcript = open(file, \"r\").read()\n",
    "    transcript = format_text(transcript)\n",
    "    words = [w.text for w in spacy_tokenizer(transcript)]\n",
    "    inputs, mapping = map_word_to_inputs([\"The cat\", \"is\", \"on\", \"the\", \"mat\"], tokenizer)\n",
    "    _, mapping = map_word_to_inputs(transcript, tokenizer)\n",
    "    \n",
    "    valid = [len(i)==2 for i in mapping.values()]\n",
    "    lexical[task] = (np.sum(valid), len(valid), np.sum(valid)/len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "differential-certificate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['21styear', 'bigbang', 'black', 'bronx', 'forgot', 'friends', 'himym', 'lucy', 'merlin', 'milkywayoriginal', 'milkywaysynonyms', 'milkywayvodka', 'notthefallintact', 'pieman', 'piemanpni', 'prettymouth', 'santa', 'seinfeld', 'shame', 'shapesphysical', 'shapessocial', 'sherlock', 'slumlordreach', 'tunnel', 'upintheair', 'vinny'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "rubber-calendar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sublexical</th>\n",
       "      <th>all</th>\n",
       "      <th>share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21styear</th>\n",
       "      <td>0.0</td>\n",
       "      <td>45573.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigbang</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2636.0</td>\n",
       "      <td>0.006829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7738.0</td>\n",
       "      <td>0.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bronx</th>\n",
       "      <td>60.0</td>\n",
       "      <td>7035.0</td>\n",
       "      <td>0.008529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forgot</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10909.0</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friends</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>0.002348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>himym</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2575.0</td>\n",
       "      <td>0.005437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lucy</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8493.0</td>\n",
       "      <td>0.000471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merlin</th>\n",
       "      <td>72.0</td>\n",
       "      <td>11594.0</td>\n",
       "      <td>0.006210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milkywayoriginal</th>\n",
       "      <td>91.0</td>\n",
       "      <td>5731.0</td>\n",
       "      <td>0.015879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milkywaysynonyms</th>\n",
       "      <td>95.0</td>\n",
       "      <td>6009.0</td>\n",
       "      <td>0.015810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milkywayvodka</th>\n",
       "      <td>89.0</td>\n",
       "      <td>5845.0</td>\n",
       "      <td>0.015227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notthefallintact</th>\n",
       "      <td>102.0</td>\n",
       "      <td>8162.0</td>\n",
       "      <td>0.012497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pieman</th>\n",
       "      <td>58.0</td>\n",
       "      <td>4869.0</td>\n",
       "      <td>0.011912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>piemanpni</th>\n",
       "      <td>51.0</td>\n",
       "      <td>5055.0</td>\n",
       "      <td>0.010089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prettymouth</th>\n",
       "      <td>12.0</td>\n",
       "      <td>10370.0</td>\n",
       "      <td>0.001157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2591.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seinfeld</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shame</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shapesphysical</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5590.0</td>\n",
       "      <td>0.000179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shapessocial</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4859.0</td>\n",
       "      <td>0.000412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sherlock</th>\n",
       "      <td>158.0</td>\n",
       "      <td>14452.0</td>\n",
       "      <td>0.010933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slumlordreach</th>\n",
       "      <td>113.0</td>\n",
       "      <td>27944.0</td>\n",
       "      <td>0.004044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunnel</th>\n",
       "      <td>25.0</td>\n",
       "      <td>19124.0</td>\n",
       "      <td>0.001307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upintheair</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2790.0</td>\n",
       "      <td>0.000358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vinny</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2638.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sublexical      all     share\n",
       "21styear                 0.0  45573.0  0.000000\n",
       "bigbang                 18.0   2636.0  0.006829\n",
       "black                    1.0   7738.0  0.000129\n",
       "bronx                   60.0   7035.0  0.008529\n",
       "forgot                   1.0  10909.0  0.000092\n",
       "friends                  6.0   2555.0  0.002348\n",
       "himym                   14.0   2575.0  0.005437\n",
       "lucy                     4.0   8493.0  0.000471\n",
       "merlin                  72.0  11594.0  0.006210\n",
       "milkywayoriginal        91.0   5731.0  0.015879\n",
       "milkywaysynonyms        95.0   6009.0  0.015810\n",
       "milkywayvodka           89.0   5845.0  0.015227\n",
       "notthefallintact       102.0   8162.0  0.012497\n",
       "pieman                  58.0   4869.0  0.011912\n",
       "piemanpni               51.0   5055.0  0.010089\n",
       "prettymouth             12.0  10370.0  0.001157\n",
       "santa                    0.0   2591.0  0.000000\n",
       "seinfeld                 0.0   2630.0  0.000000\n",
       "shame                    0.0   2625.0  0.000000\n",
       "shapesphysical           1.0   5590.0  0.000179\n",
       "shapessocial             2.0   4859.0  0.000412\n",
       "sherlock               158.0  14452.0  0.010933\n",
       "slumlordreach          113.0  27944.0  0.004044\n",
       "tunnel                  25.0  19124.0  0.001307\n",
       "upintheair               1.0   2790.0  0.000358\n",
       "vinny                    0.0   2638.0  0.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(lexical).T\n",
    "df.columns = (\"lexical\", \"all\", \"share\")\n",
    "df[\"sublexical\"] = df[\"all\"] - df[\"lexical\"]\n",
    "df[\"share\"] = 1-df[\"share\"]\n",
    "df[[\"sublexical\", \"all\", \"share\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_embeddings(words, model_name=\"gpt2\", agg=\"mean\"):\n",
    "\n",
    "    # Load\n",
    "    # words = events.word_raw.values\n",
    "    assert agg in [\"sum\", \"mean\", \"last\"]\n",
    "\n",
    "    print(f\"Loading model {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Inputs\n",
    "    print(\"Processed words\", words[:5])\n",
    "    inputs, mapping = map_word_to_inputs(words, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hasson",
   "language": "python",
   "name": "hasson"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
