{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "several-harrison",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/ccaucheteux/hasson-syntaxe-vs-semantics\n"
     ]
    }
   ],
   "source": [
    "cd /private/home/ccaucheteux/hasson-syntaxe-vs-semantics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "diagnostic-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "quick-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "import nilearn\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "perfect-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.stats import fdr_correction\n",
    "from scipy.stats import wilcoxon \n",
    "\n",
    "def get_pvals(r, corrected=True, alpha=0.05):\n",
    "    # r of shape (dim, n_samples)\n",
    "    pvals = [wilcoxon(x)[1] for x in r]\n",
    "    if corrected:\n",
    "        pvals = fdr_correction(pvals, alpha=alpha, method='indep')[0]\n",
    "    else:\n",
    "        pvals = np.array(pvals)<=alpha\n",
    "    return pvals\n",
    "\n",
    "def set_ticks(ax, x_values = None, y_values = None):\n",
    "    if x_values is not None:\n",
    "        ax.set_xticks(x_values)\n",
    "        ax.set_xticklabels([f\"{x:.2f}\".replace(\"0.\", \".\") for x in x_values])\n",
    "    if y_values is not None:\n",
    "        ax.set_yticks(y_values)\n",
    "        ax.set_yticklabels([f\"{x:.2f}\".replace(\"0.\", \".\") for x in y_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "prepared-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [\n",
    "    'forgot',\n",
    "    'black',\n",
    "    'merlin',\n",
    "    'sherlock',\n",
    "    #'shapessocial',\n",
    "    #'shapesphysical',\n",
    "    'piemanpni',\n",
    "    'bronx',\n",
    "    #'21styear',\n",
    "    #'prettymouth',\n",
    "    'slumlordreach'\n",
    "]\n",
    "\n",
    "# bronx, piemanpni, black, and forgot\n",
    "\n",
    "FEATS = [\n",
    "    \"3_phone_features\",\n",
    "    'sum-gpt2-0',\n",
    "    'sum-gpt2-9',\n",
    "    #'sum-gpt2-6',\n",
    "]\n",
    "\n",
    "ALL_FEATS = FEATS.copy()\n",
    "ALL_FEATS += [\n",
    "    f\"sum-gpt2-{i}\" for i in range(13)\n",
    "]\n",
    "ALL_FEATS = np.unique(ALL_FEATS)\n",
    "\n",
    "LABELS = {\n",
    "    '3_phone_features':\"Phonological\", \n",
    "    'phone_sum-gpt2-0':\"Word embedding\",\n",
    "    'phone_sum-gpt2-9.equiv-random-mean-10':\"GPT29 - syntax\",\n",
    "    'phone_sum-gpt2-9':\"GPT2 (layer 9)\",\n",
    "    'sum-gpt2-9':\"GPT2 (layer 9)\",\n",
    "\n",
    "    'sum-gpt2-0':\"Word embedding\",\n",
    "    'phone_sum-gpt2-9.shuffle_in_sentence':\"GPT29 - scrambled sentences\", \n",
    "    'wordpos':\"Word position\"\n",
    "}\n",
    "for i in np.arange(1, 13):\n",
    "    if i not in [9]:\n",
    "        LABELS[f'phone_sum-gpt2-{i}'] = f'GPT2 (layer {i})'\n",
    "        LABELS[f'sum-gpt2-{i}'] = f'GPT2 (layer {i})'\n",
    "\n",
    "COLORS = {\n",
    "    '3_phone_features':\"b\", \n",
    "    'phone_sum-gpt2-0':\"g\",\n",
    "    'sum-gpt2-0':\"g\",\n",
    "    'phone_sum-gpt2-9':\"r\",\n",
    "}\n",
    "palette = sns.color_palette(\"Reds\", 13)\n",
    "\n",
    "for i in np.arange(1, 13):\n",
    "    if i not in [0]:\n",
    "        COLORS[f'phone_sum-gpt2-{i}'] = palette[i]\n",
    "        COLORS[f'sum-gpt2-{i}'] = palette[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-buyer",
   "metadata": {},
   "source": [
    "# Gather results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "appreciated-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"multisubjects-0130\"\n",
    "EXP_NAME = \"multisubjects-0130\"\n",
    "EXP_NAME = \"concat-multisubjects-0201-valid\"\n",
    "EXP_NAME = \"100-concat-multisubjects-0206-wordemb\"\n",
    "EXP_NAME = \"concat-single-task-0206\"\n",
    "#EXP_NAME = \"regressout-multisubjects-0201-valid\"\n",
    "CONCAT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "nominated-danger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /public/apps/anaconda3/2020.11/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "\u001b[0m\u001b[01;34m0222-gpt2-errors-multisubjects\u001b[0m/          \u001b[01;34mmedian-multisubjects-0130\u001b[0m/\r\n",
      "\u001b[01;34m0222-gpt2-errors-multisubjects-5folds\u001b[0m/   \u001b[01;34mmultisubjects\u001b[0m/\r\n",
      "\u001b[01;34m0223-gpt2-errors-multisubjects-10folds\u001b[0m/  \u001b[01;34mmultisubjects-0130\u001b[0m/\r\n",
      "\u001b[01;34m0223-gpt2-errors-singlesubjects\u001b[0m/         \u001b[01;34mmultisubjects-concat\u001b[0m/\r\n",
      "\u001b[01;34m100-concat-multisubjects-0206-wordemb\u001b[0m/   \u001b[01;34mmultisubjects-control\u001b[0m/\r\n",
      "\u001b[01;34m200-concat-multisubjects-0201-valid\u001b[0m/     \u001b[01;34mmultisubjects-controls\u001b[0m/\r\n",
      "\u001b[01;34mconcat-multisubjects-0130\u001b[0m/               \u001b[01;34mregressout-multisubjects-0201-valid\u001b[0m/\r\n",
      "\u001b[01;34mconcat-multisubjects-0201\u001b[0m/               \u001b[01;34mregressout-single-subjects-0206\u001b[0m/\r\n",
      "\u001b[01;34mconcat-multisubjects-0201-newformat\u001b[0m/     \u001b[01;34mregressout-single-subjects-0209\u001b[0m/\r\n",
      "\u001b[01;34mconcat-multisubjects-0201-seeds\u001b[0m/         \u001b[01;34msingle-subject-multitasks\u001b[0m/\r\n",
      "\u001b[01;34mconcat-multisubjects-0201-valid\u001b[0m/         \u001b[01;34msingle-subject-multitasks-0129\u001b[0m/\r\n",
      "\u001b[01;34mconcat-single-subjects-0201\u001b[0m/             \u001b[01;34msingle-subject-multitasks-concat\u001b[0m/\r\n",
      "\u001b[01;34mconcat-single-task-0201\u001b[0m/                 \u001b[01;34msingle-subjects-0130\u001b[0m/\r\n",
      "\u001b[01;34mconcat-single-task-0206\u001b[0m/                 \u001b[01;34mtest\u001b[0m/\r\n",
      "\u001b[01;34mgpt2-single-subjects-multitasks\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls $paths.scores/ #concat-multisubjects-0201-newformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "prescribed-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "italian-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "def gather_scores(scores):\n",
    "    names = {}\n",
    "    result = {\"L\": defaultdict(lambda x: []), \n",
    "             \"R\": defaultdict(lambda x: [])}\n",
    "    for hemi in [\"L\", \"R\"]:\n",
    "        for d in scores:\n",
    "            if hemi in d:\n",
    "                for k, v in d[hemi].items():\n",
    "                    result[hemi][k].append(v)\n",
    "        \n",
    "        # Names\n",
    "        names[hemi] = result[hemi].keys()\n",
    "        names[hemi] = [p.name.split(\".pth\")[0] for p in names[hemi]]\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        scores[hemi] = np.stack(list(result[hemi].values()))\n",
    "    \n",
    "    assert (np.array(names[\"L\"]) == np.array(names[\"L\"])).all()\n",
    "    scores = np.stack([scores[\"L\"], scores[\"R\"]], axis=1)\n",
    "    names = names[\"L\"].copy()\n",
    "    return scores, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "instructional-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_dataset import get_task_df\n",
    "df_path = pd.read_csv(paths.scores / EXP_NAME / \"results_path.csv\")\n",
    "df_path[\"is_file\"] = df_path.save_file.apply(lambda x : Path(x).is_file())\n",
    "df_path = df_path.query(\"is_file\")\n",
    "df_path = df_path.sort_values(\"feature_file\")\n",
    "df_path = df_path.drop_duplicates()\n",
    "\n",
    "df = get_task_df()\n",
    "\n",
    "# Add duration \n",
    "duration = json.load(open(paths.event_meta_path))\n",
    "duration = {k:v[\"duration\"] for _, v1 in duration.items() for k, v in v1.items()}\n",
    "duration[\"slumlordreach\"] = duration[\"slumlord\"] + duration[\"reach\"]\n",
    "duration = pd.DataFrame(duration, index=[\"duration\"]).T\n",
    "duration = duration.reset_index().rename(columns={\"index\":\"audio_task\"})\n",
    "df = pd.merge(df, duration, on=\"audio_task\", how=\"left\")\n",
    "\n",
    "\n",
    "df_path = pd.merge(df_path, df, left_on=[\"subject\", \"task\"], right_on=[\"subject\", \"audio_task\"], how=\"left\")\n",
    "df_path[\"feat\"] = [Path(p).name.split(\".pth\")[0] for p in df_path.feature_file]\n",
    "\n",
    "df_path = df_path.query(\"feat in @ALL_FEATS and task in @TASKS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "immediate-center",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = df_path.save_file.apply(lambda x: np.load(x, allow_pickle=True).item())\n",
    "idx = np.where([i is not None for i in scores])\n",
    "scores = scores.iloc[idx]\n",
    "df_path = df_path.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "complete-wright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3318, 40962)"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = scores.copy()\n",
    "scores = np.stack([np.stack([list(val[hemi].values())[0] for val in result]) for hemi in [\"L\", \"R\"]])\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "robust-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_parc(xyz, label, n, axis='y'):\n",
    "\n",
    "    axes = dict(x=0, y=1, z=2)\n",
    "\n",
    "    m = xyz[:, axes[axis]].min()\n",
    "    M = xyz[:, axes[axis]].max()\n",
    "    bounds = (M-m) * np.linspace(0, 1., 1+n) + m\n",
    "\n",
    "    groups = np.digitize(xyz[:, axes[axis]], \n",
    "                         bounds)\n",
    "    \n",
    "    labels = list()\n",
    "    for group_id, _ in enumerate(bounds):\n",
    "        label_ = label.copy()\n",
    "        label_.name = f'{group_id}_' + label.name\n",
    "        label_.vertices = label.vertices[groups==group_id]\n",
    "        labels.append(label_)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bored-nowhere",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "subjects_dir = \"../narratives/derivatives/freesurfer/\"\n",
    "surf = 'pial'\n",
    "surf = Path(subjects_dir) / 'fsaverage6' / 'surf' / f'%s.{surf}'\n",
    "xyz = {\"rh\": nib.freesurfer.read_geometry(str(surf) % \"rh\")[0],\n",
    "       \"lh\": nib.freesurfer.read_geometry(str(surf) % \"lh\")[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "discrete-creator",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_labels(all_labels, areas, subjects_dir=\"\", surf = 'pial'):\n",
    "    surf = Path(subjects_dir) / 'fsaverage6' / 'surf' / f'%s.{surf}'\n",
    "    xyz = {\"rh\": nib.freesurfer.read_geometry(str(surf) % \"rh\")[0],\n",
    "           \"lh\": nib.freesurfer.read_geometry(str(surf) % \"lh\")[0]}\n",
    "    new_labels = []\n",
    "    for label in all_labels:\n",
    "        if len(label.vertices) > 400:\n",
    "            n = len(label.vertices) // 400\n",
    "            hemi = label.hemi\n",
    "            new = split_parc(xyz[hemi][label.vertices], label, n)\n",
    "            new_labels.extend(new)\n",
    "        else:\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "vanilla-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_labels(all_labels, subjects_dir=\"\", surf = 'pial'):\n",
    "    \n",
    "    areas = [\"-\".join(l.name.split(\"-\")[:-1]) for l in all_labels] # if \"Networks\" in i]\n",
    "    areas = np.unique(areas)\n",
    "    \n",
    "    surf = Path(subjects_dir) / 'fsaverage6' / 'surf' / f'%s.{surf}'\n",
    "    xyz = {\"rh\": nib.freesurfer.read_geometry(str(surf) % \"rh\")[0],\n",
    "           \"lh\": nib.freesurfer.read_geometry(str(surf) % \"lh\")[0]}\n",
    "    new_labels = []\n",
    "    for area in areas:\n",
    "        labels = [l for l in all_labels if l.name in [area+\"-lh\", area+\"-rh\"]]\n",
    "        assert len(labels)==2\n",
    "        n = max([len(l.vertices) for l in labels])\n",
    "        if n > 350:\n",
    "            n = n//350\n",
    "            for hemi, l in zip([\"lh\", \"rh\"], labels):\n",
    "                new = split_parc(xyz[hemi][l.vertices], l, n)\n",
    "                new_labels.extend(new)\n",
    "        else:\n",
    "            new_labels.extend(labels)\n",
    "            \n",
    "    \"\"\"for label in all_labels:\n",
    "        if len(label.vertices) > 500:\n",
    "            n = len(label.vertices) // 500\n",
    "            hemi = label.hemi\n",
    "            new = split_parc(xyz[hemi][label.vertices], label, n)\n",
    "            new_labels.extend(new)\n",
    "        else:\n",
    "            new_labels.append(label)\"\"\"\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "consistent-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import mne\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "color-saint",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-705-1710869f4341>:21: RuntimeWarning: Mean of empty slice\n",
      "  scores_rois[h, :, i] = np.nanmean(scores[h, :, rois[f\"{area}-{hemi}\"]], 0)\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "subjects_dir = \"../narratives/derivatives/freesurfer/\"\n",
    "all_labels = mne.read_labels_from_annot('fsaverage6', parc='aparc.a2009s', #parc='Yeo2011_17Networks_N1000', \n",
    "                                            subjects_dir=subjects_dir, verbose=False)\n",
    "\n",
    "#areas = [\"-\".join(l.name.split(\"-\")[:-1]) for l in all_labels]\n",
    "\n",
    "all_labels = split_labels(all_labels, subjects_dir=subjects_dir)\n",
    "\n",
    "rois = {l.name : l.vertices for l in all_labels}\n",
    "rois_colors = {\"-\".join(l.name.split(\"-\")[:-1]) : l.color for l in all_labels if \"lh\" in l.name}\n",
    "\n",
    "#areas = [i.split(\"-\")[0] for i in rois.keys() if \"Networks\" in i]\n",
    "areas = [\"-\".join(i.split(\"-\")[:-1]) for i in rois.keys()] # if \"Networks\" in i]\n",
    "areas = np.unique(areas)\n",
    "\n",
    "# Scores (75 regions)\n",
    "scores_rois = np.zeros((*scores.shape[:-1], len(areas)))\n",
    "for i, area in enumerate(areas):\n",
    "    for h, hemi in enumerate([\"lh\", \"rh\"]):\n",
    "        scores_rois[h, :, i] = np.nanmean(scores[h, :, rois[f\"{area}-{hemi}\"]], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /private/home/ccaucheteux/hasson-syntaxe-vs-semantics/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import paths\n",
    "\n",
    "from nilearn import plotting\n",
    "import nilearn\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from mne.stats import fdr_correction\n",
    "from scipy.stats import wilcoxon \n",
    "\n",
    "def get_pvals(r, corrected=True, alpha=0.05):\n",
    "    # r of shape (dim, n_samples)\n",
    "    pvals = [wilcoxon(x)[1] for x in r]\n",
    "    if corrected:\n",
    "        pvals = fdr_correction(pvals, alpha=alpha, method='indep')[0]\n",
    "    else:\n",
    "        pvals = np.array(pvals)<=alpha\n",
    "    return pvals\n",
    "\n",
    "def set_ticks(ax, x_values = None, y_values = None):\n",
    "    if x_values is not None:\n",
    "        ax.set_xticks(x_values)\n",
    "        ax.set_xticklabels([f\"{x:.2f}\".replace(\"0.\", \".\") for x in x_values])\n",
    "    if y_values is not None:\n",
    "        ax.set_yticks(y_values)\n",
    "        ax.set_yticklabels([f\"{x:.2f}\".replace(\"0.\", \".\") for x in y_values])\n",
    "\n",
    "TASKS = [\n",
    "    'forgot',\n",
    "    'black',\n",
    "    'merlin',\n",
    "    'sherlock',\n",
    "    #'shapessocial',\n",
    "    #'shapesphysical',\n",
    "    'piemanpni',\n",
    "    'bronx',\n",
    "    #'21styear',\n",
    "    #'prettymouth',\n",
    "    'slumlordreach'\n",
    "]\n",
    "\n",
    "# bronx, piemanpni, black, and forgot\n",
    "\n",
    "FEATS = [\n",
    "    \"3_phone_features\",\n",
    "    'sum-gpt2-0',\n",
    "    'sum-gpt2-9',\n",
    "    #'sum-gpt2-6',\n",
    "]\n",
    "\n",
    "ALL_FEATS = FEATS.copy()\n",
    "ALL_FEATS += [\n",
    "    f\"sum-gpt2-{i}\" for i in range(13)\n",
    "]\n",
    "ALL_FEATS = np.unique(ALL_FEATS)\n",
    "\n",
    "LABELS = {\n",
    "    '3_phone_features':\"Phonological\", \n",
    "    'phone_sum-gpt2-0':\"Word embedding\",\n",
    "    'phone_sum-gpt2-9.equiv-random-mean-10':\"GPT29 - syntax\",\n",
    "    'phone_sum-gpt2-9':\"GPT2 (layer 9)\",\n",
    "    'sum-gpt2-9':\"GPT2 (layer 9)\",\n",
    "\n",
    "    'sum-gpt2-0':\"Word embedding\",\n",
    "    'phone_sum-gpt2-9.shuffle_in_sentence':\"GPT29 - scrambled sentences\", \n",
    "    'wordpos':\"Word position\"\n",
    "}\n",
    "for i in np.arange(1, 13):\n",
    "    if i not in [9]:\n",
    "        LABELS[f'phone_sum-gpt2-{i}'] = f'GPT2 (layer {i})'\n",
    "        LABELS[f'sum-gpt2-{i}'] = f'GPT2 (layer {i})'\n",
    "\n",
    "COLORS = {\n",
    "    '3_phone_features':\"b\", \n",
    "    'phone_sum-gpt2-0':\"g\",\n",
    "    'sum-gpt2-0':\"g\",\n",
    "    'phone_sum-gpt2-9':\"r\",\n",
    "}\n",
    "palette = sns.color_palette(\"Reds\", 13)\n",
    "\n",
    "for i in np.arange(1, 13):\n",
    "    if i not in [0]:\n",
    "        COLORS[f'phone_sum-gpt2-{i}'] = palette[i]\n",
    "        COLORS[f'sum-gpt2-{i}'] = palette[i]\n",
    "\n",
    "# Gather results\n",
    "\n",
    "EXP_NAME = \"multisubjects-0130\"\n",
    "EXP_NAME = \"multisubjects-0130\"\n",
    "EXP_NAME = \"concat-multisubjects-0201-valid\"\n",
    "EXP_NAME = \"100-concat-multisubjects-0206-wordemb\"\n",
    "EXP_NAME = \"concat-single-task-0206\"\n",
    "#EXP_NAME = \"regressout-multisubjects-0201-valid\"\n",
    "CONCAT = True\n",
    "\n",
    "ls $paths.scores/ #concat-multisubjects-0201-newformat\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "def gather_scores(scores):\n",
    "    names = {}\n",
    "    result = {\"L\": defaultdict(lambda x: []), \n",
    "             \"R\": defaultdict(lambda x: [])}\n",
    "    for hemi in [\"L\", \"R\"]:\n",
    "        for d in scores:\n",
    "            if hemi in d:\n",
    "                for k, v in d[hemi].items():\n",
    "                    result[hemi][k].append(v)\n",
    "        \n",
    "        # Names\n",
    "        names[hemi] = result[hemi].keys()\n",
    "        names[hemi] = [p.name.split(\".pth\")[0] for p in names[hemi]]\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        scores[hemi] = np.stack(list(result[hemi].values()))\n",
    "    \n",
    "    assert (np.array(names[\"L\"]) == np.array(names[\"L\"])).all()\n",
    "    scores = np.stack([scores[\"L\"], scores[\"R\"]], axis=1)\n",
    "    names = names[\"L\"].copy()\n",
    "    return scores, names\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_dataset import get_task_df\n",
    "df_path = pd.read_csv(paths.scores / EXP_NAME / \"results_path.csv\")\n",
    "df_path[\"is_file\"] = df_path.save_file.apply(lambda x : Path(x).is_file())\n",
    "df_path = df_path.query(\"is_file\")\n",
    "df_path = df_path.sort_values(\"feature_file\")\n",
    "df_path = df_path.drop_duplicates()\n",
    "\n",
    "df = get_task_df()\n",
    "\n",
    "# Add duration \n",
    "duration = json.load(open(paths.event_meta_path))\n",
    "duration = {k:v[\"duration\"] for _, v1 in duration.items() for k, v in v1.items()}\n",
    "duration[\"slumlordreach\"] = duration[\"slumlord\"] + duration[\"reach\"]\n",
    "duration = pd.DataFrame(duration, index=[\"duration\"]).T\n",
    "duration = duration.reset_index().rename(columns={\"index\":\"audio_task\"})\n",
    "df = pd.merge(df, duration, on=\"audio_task\", how=\"left\")\n",
    "\n",
    "\n",
    "df_path = pd.merge(df_path, df, left_on=[\"subject\", \"task\"], right_on=[\"subject\", \"audio_task\"], how=\"left\")\n",
    "df_path[\"feat\"] = [Path(p).name.split(\".pth\")[0] for p in df_path.feature_file]\n",
    "\n",
    "df_path = df_path.query(\"feat in @ALL_FEATS and task in @TASKS\")\n",
    "\n",
    "scores = df_path.save_file.apply(lambda x: np.load(x, allow_pickle=True).item())\n",
    "idx = np.where([i is not None for i in scores])\n",
    "scores = scores.iloc[idx]\n",
    "df_path = df_path.iloc[idx]\n",
    "\n",
    "result = scores.copy()\n",
    "scores = np.stack([np.stack([list(val[hemi].values())[0] for val in result]) for hemi in [\"L\", \"R\"]])\n",
    "scores.shape\n",
    "\n",
    "def split_parc(xyz, label, n, axis='y'):\n",
    "\n",
    "    axes = dict(x=0, y=1, z=2)\n",
    "\n",
    "    m = xyz[:, axes[axis]].min()\n",
    "    M = xyz[:, axes[axis]].max()\n",
    "    bounds = (M-m) * np.linspace(0, 1., 1+n) + m\n",
    "\n",
    "    groups = np.digitize(xyz[:, axes[axis]], \n",
    "                         bounds)\n",
    "    \n",
    "    labels = list()\n",
    "    for group_id, _ in enumerate(bounds):\n",
    "        label_ = label.copy()\n",
    "        label_.name = f'{group_id}_' + label.name\n",
    "        label_.vertices = label.vertices[groups==group_id]\n",
    "        labels.append(label_)\n",
    "    return labels\n",
    "\n",
    "\n",
    "subjects_dir = \"../narratives/derivatives/freesurfer/\"\n",
    "surf = 'pial'\n",
    "surf = Path(subjects_dir) / 'fsaverage6' / 'surf' / f'%s.{surf}'\n",
    "xyz = {\"rh\": nib.freesurfer.read_geometry(str(surf) % \"rh\")[0],\n",
    "       \"lh\": nib.freesurfer.read_geometry(str(surf) % \"lh\")[0]}\n",
    "\n",
    "def split_labels(all_labels, areas, subjects_dir=\"\", surf = 'pial'):\n",
    "    surf = Path(subjects_dir) / 'fsaverage6' / 'surf' / f'%s.{surf}'\n",
    "    xyz = {\"rh\": nib.freesurfer.read_geometry(str(surf) % \"rh\")[0],\n",
    "           \"lh\": nib.freesurfer.read_geometry(str(surf) % \"lh\")[0]}\n",
    "    new_labels = []\n",
    "    for label in all_labels:\n",
    "        if len(label.vertices) > 400:\n",
    "            n = len(label.vertices) // 400\n",
    "            hemi = label.hemi\n",
    "            new = split_parc(xyz[hemi][label.vertices], label, n)\n",
    "            new_labels.extend(new)\n",
    "        else:\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def split_labels(all_labels, subjects_dir=\"\", surf = 'pial'):\n",
    "    \n",
    "    areas = [\"-\".join(l.name.split(\"-\")[:-1]) for l in all_labels] # if \"Networks\" in i]\n",
    "    areas = np.unique(areas)\n",
    "    \n",
    "    surf = Path(subjects_dir) / 'fsaverage6' / 'surf' / f'%s.{surf}'\n",
    "    xyz = {\"rh\": nib.freesurfer.read_geometry(str(surf) % \"rh\")[0],\n",
    "           \"lh\": nib.freesurfer.read_geometry(str(surf) % \"lh\")[0]}\n",
    "    new_labels = []\n",
    "    for area in areas:\n",
    "        labels = [l for l in all_labels if l.name in [area+\"-lh\", area+\"-rh\"]]\n",
    "        assert len(labels)==2\n",
    "        n = max([len(l.vertices) for l in labels])\n",
    "        if n > 350:\n",
    "            n = n//350\n",
    "            for hemi, l in zip([\"lh\", \"rh\"], labels):\n",
    "                new = split_parc(xyz[hemi][l.vertices], l, n)\n",
    "                new_labels.extend(new)\n",
    "        else:\n",
    "            new_labels.extend(labels)\n",
    "            \n",
    "    \"\"\"for label in all_labels:\n",
    "        if len(label.vertices) > 500:\n",
    "            n = len(label.vertices) // 500\n",
    "            hemi = label.hemi\n",
    "            new = split_parc(xyz[hemi][label.vertices], label, n)\n",
    "            new_labels.extend(new)\n",
    "        else:\n",
    "            new_labels.append(label)\"\"\"\n",
    "    return new_labels\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import mne\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mne\n",
    "subjects_dir = \"../narratives/derivatives/freesurfer/\"\n",
    "all_labels = mne.read_labels_from_annot('fsaverage6', parc='aparc.a2009s', #parc='Yeo2011_17Networks_N1000', \n",
    "                                            subjects_dir=subjects_dir, verbose=False)\n",
    "\n",
    "#areas = [\"-\".join(l.name.split(\"-\")[:-1]) for l in all_labels]\n",
    "\n",
    "all_labels = split_labels(all_labels, subjects_dir=subjects_dir)\n",
    "\n",
    "rois = {l.name : l.vertices for l in all_labels}\n",
    "rois_colors = {\"-\".join(l.name.split(\"-\")[:-1]) : l.color for l in all_labels if \"lh\" in l.name}\n",
    "\n",
    "#areas = [i.split(\"-\")[0] for i in rois.keys() if \"Networks\" in i]\n",
    "areas = [\"-\".join(i.split(\"-\")[:-1]) for i in rois.keys()] # if \"Networks\" in i]\n",
    "areas = np.unique(areas)\n",
    "\n",
    "# Scores (75 regions)\n",
    "scores_rois = np.zeros((*scores.shape[:-1], len(areas)))\n",
    "for i, area in enumerate(areas):\n",
    "    for h, hemi in enumerate([\"lh\", \"rh\"]):\n",
    "        scores_rois[h, :, i] = np.nanmean(scores[h, :, rois[f\"{area}-{hemi}\"]], 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hasson",
   "language": "python",
   "name": "hasson"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
